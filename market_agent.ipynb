{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3cd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langgraph openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a64de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade --force-reinstall langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cea81531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # .env 파일에서 환경변수 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67a15f",
   "metadata": {},
   "source": [
    "#### Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "222d76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d2aa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearch(max_results=5)\n",
    "\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14424467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?', 'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial', 'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.', 'score': 0.91137725, 'raw_content': 'Published Time: 2024-06-26T21:00:00.000Z\\nLangGraph Tutorial: What Is LangGraph and How to Use It? | DataCamp\\n ##### Get 50% off unlimited learning Buy Now\\nSkip to main content\\n\\nEN\\nEnglishEspañolBetaPortuguêsBetaDeutschBetaFrançaisBeta\\n\\nFound an Error?\\nLog InGet Started\\nTutorials\\nBlogs\\nTutorials\\ndocs\\nPodcasts\\nCheat Sheets\\ncode-alongs\\nNewsletter\\n\\nCategory\\nCategory\\nTechnologies\\nDiscover content by tools and technology\\nAI AgentsArtificial IntelligenceAWSAzureBusiness IntelligenceChatGPTDatabricksdbtDockerExcelGenerative AIGitGoogle Cloud PlatformHugging FaceJavaJuliaKafkaKubernetesLarge Language ModelsOpenAIPostgreSQLPower BIPySparkPythonRScalaSnowflakeSpreadsheetsSQLSQLiteTableau\\nCategory\\nTopics\\nDiscover content by data science topics\\nAI for BusinessBig DataCareer ServicesCloudData AnalysisData EngineeringData LiteracyData ScienceData VisualizationDataLabDeep LearningMachine LearningMLOpsNatural Language Processing\\nBrowse Courses\\ncategory\\n\\nHome\\nTutorials\\n\\nLangGraph Tutorial: What Is LangGraph and How to Use It?\\nLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\\nContents\\nJun 26, 2024 \\xa0· 12 min read\\nContents\\n\\nWhat Is LangGraph?\\nGraph structure\\nState management\\n\\nCoordination\\n\\n\\nWhy LangGraph?\\n\\nSimplified development\\nFlexibility\\nScalability\\n\\nFault tolerance\\n\\n\\nGetting Started With LangGraph\\n\\nInstallation\\n\\nBasic Concepts\\n\\n\\nBuilding a Simple LangGraph Application\\n\\nStep 1: Define the StateGraph\\nStep 2: Initialize an LLM and add it as a Chatbot node\\nStep 3: Set edges\\n\\nStep 5: Run the chatbot\\n\\n\\nAdvanced LangGraph Features\\n\\nCustom node types\\nEdge types\\n\\nError handling\\n\\n\\nReal-World Applications of LangGraph\\n\\nChatbots\\nAutonomous agents\\nMulti-Agent systems\\nWorkflow automation tools\\nRecommendation systems\\n\\nPersonalized learning environments\\n\\n\\nConclusion\\n\\n\\nTraining more people?\\nGet your team access to the full DataCamp for business platform.\\nFor BusinessFor a bespoke solution book a demo.\\nImagine you\\'re building a complex, multi-agent large language model (LLM) application. It\\'s exciting, but it comes with challenges: managing the state of various agents, coordinating their interactions, and handling errors effectively. This is where LangGraph can help.\\nLangGraph is a library within the LangChain ecosystem designed to tackle these challenges head-on. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner.\\nIt simplifies the development process by enabling the creation of cyclical graphs, which are essential for developing agent runtimes. With LangGraph, we can easily build robust, scalable, and flexible multi-agent systems.\\nIf you want to learn more about the LangChain ecosystem, I recommend this introduction to LangChain.\\nWhat Is LangGraph?\\nLangGraph enables us to create stateful, multi-actor applications utilizing LLMs as easily as possible. It extends the capabilities of LangChain, introducing the ability to create and manage cyclical graphs, which are pivotal for developing sophisticated agent runtimes. The core concepts of LangGraph include: graph structure, state management, and coordination.\\nGraph structure\\nImagine your application as a directed graph. In LangGraph, each node represents an LLM agent, and the edges are the communication channels between these agents. This structure allows for clear and manageable workflows, where each agent performs specific tasks and passes information to other agents as needed.\\nState management\\nOne of LangGraph\\'s standout features is its automatic state management. This feature enables us to track and persist information across multiple interactions. As agents perform their tasks, the state is dynamically updated, ensuring the system maintains context and responds appropriately to new inputs.\\nCoordination\\nLangGraph ensures agents execute in the correct order and that necessary information is exchanged seamlessly. This coordination is vital for complex applications where multiple agents need to work together to achieve a common goal. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination.\\nWhy LangGraph?\\nAs I mentioned above, LangGraph offers several significant advantages for developers working with complex LLM applications. Here are some of the real-world benefits LangGraph offers.\\nSimplified development\\nLangGraph abstracts away the complexities associated with state management and agent coordination. This means developers can define their workflows and logic without worrying about the underlying mechanisms that ensure data consistency and proper execution order. This simplification accelerates the development process and reduces the likelihood of errors. It’s a game-changer!\\nFlexibility\\nWith LangGraph, developers have the flexibility to define their own agent logic and communication protocols. This allows for highly customized applications tailored to specific use cases. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. It’s all about giving you the power to create.\\nScalability\\nLangGraph is built to support the execution of large-scale multi-agent applications. Its robust architecture can handle a high volume of interactions and complex workflows, enabling the development of scalable systems that can grow with your needs. This makes it suitable for enterprise-level applications and scenarios where performance and reliability are critical.\\nFault tolerance\\nReliability is a core consideration in the design of LangGraph. The library includes mechanisms for gracefully handling errors, ensuring that your application can continue to operate even when individual agents encounter issues. This fault tolerance is essential for maintaining the stability and robustness of complex multi-agent systems. Peace of mind is just a feature away.\\nGetting Started With LangGraph\\nLet’s see how we can set up LangGraph and what the basic concepts are.\\nInstallation\\nTo install LangGraph, you can use pip:\\npip install -U langgraph\\nPowered By \\nWas this helpful? Yes No\\nBasic Concepts\\nNodes: Nodes represent units of work within your LangGraph. They are typically Python functions that perform a specific task, such as:\\n\\nInteracting with an LLM\\nCalling a tool or API\\nPerforming some data manipulation\\nReceiving user input\\nExecuting business logic\\n\\nIn LangGraph, you can add nodes using the graph.add_node(name, value) syntax.\\nEdges: Edges are communication channels between nodes. They define the flow of information and the order of execution. You can add edges using the graph.add_edge(node1, node2) syntax.\\nState: The state is a central object updated over time by the nodes in the graph. It manages the internal state of your application and can be overridden or added to, depending on the application\\'s requirements. This state can hold things such as:\\n\\nConversation history: A list of messages between the agent and the user.\\nContextual data: Information relevant to the current task or interaction.\\nInternal variables: Flags, counters, or other variables to track the agent\\'s progress and behavior.\\n\\nBuilding a Simple LangGraph Application\\nHere’s a step-by-step example of creating a basic chatbot application using LangGraph.\\nStep 1: Define the StateGraph\\nDefine a StateGraph object to structure the chatbot as a state machine. The State is a class object defined with a single key messages of type List and uses the add_messages() function to append new messages rather than overwrite them.\\nfrom typing import Annotated\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.graph.message import add_messages\\nclass State(TypedDict):\\n    # messages have the type \"list\".\\n    # The add_messages function appends messages to the list, rather than overwriting them\\n    messages: Annotated[list, add_messages]\\ngraph_builder = StateGraph(State)\\nPowered By \\nWas this helpful? Yes No\\nStep 2: Initialize an LLM and add it as a Chatbot node\\nHere, we initialize the AzureChatOpenAI model and create a simple chatbot function that takes in the state messages as input and generates a message response (which is subsequently appended to the state).\\nThis chatbot function is added as a node named “chatbot” to the graph.\\n```\\nfrom langchain_openai import AzureChatOpenAI\\nllm = AzureChatOpenAI(\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\\n)\\ndef chatbot(state: State):\\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\\n‘’’The first argument is the unique node name\\nThe second argument is the function or object that will be called whenever the node is used.’’’\\ngraph_builder.add_node(\"chatbot\", chatbot)\\n```\\nPowered By \\nStep 3: Set edges\\nSince we are building a simple chatbot, we set the chatbot node as both the entry and finish points of the graph to indicate where to start and end the process.\\n```\\nSet entry and finish points\\ngraph_builder.set_entry_point(\"chatbot\")\\ngraph_builder.set_finish_point(\"chatbot\")\\n```\\nPowered By \\nWas this helpful? Yes No\\nStep 4: Compile and Visualize the Graph\\nCompile the graph to create a CompiledGraph object, and optionally, we can visualize the graph structure using the code below:\\ngraph = graph_builder.compile()\\nfrom IPython.display import Image, display\\ntry:\\n    display(Image(graph.get_graph().draw_mermaid_png()))\\nexcept Exception:\\n    pass\\nPowered By \\nWas this helpful? Yes No\\n\\nStep 5: Run the chatbot\\nFinally, we implement a loop to continuously prompt the user for input, process it through the graph, and print the assistant\\'s response. The loop exits when the user types \"quit\", \"exit\", or \"q\".\\n```\\nRun the chatbot\\nwhile True:\\n    user_input = input(\"User: \")\\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\\n        print(\"Goodbye!\")\\n        break\\n    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\\n        for value in event.values():\\n            print(\"Assistant:\", value[\"messages\"][-1].content)\\n```\\nPowered By \\nWas this helpful? Yes No\\nAdvanced LangGraph Features\\nNow that we covered the basics, let’s take a look at some advanced features.\\nCustom node types\\nLangGraph allows you to create custom node types to implement complex agent logic. This provides flexibility and control over your application\\'s behavior.\\nfrom typing import Annotated\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langgraph.graph import StateGraph\\nfrom langgraph.graph.message import add_messages\\nclass MyCustomNode:\\n    def __init__(self, llm):\\n        self.llm = llm\\n    def __call__(self, state):\\n        # Implement your custom logic here\\n        # Access the state and perform actions\\n        messages = state[\"messages\"]\\n        response = self.llm.invoke(messages)\\n        return {\"messages\": [response]}\\ngraph_builder = StateGraph(State)\\nllm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\\ncustom_node = MyCustomNode(llm)\\ngraph_builder.add_node(\"custom_node\", custom_node)\\nPowered By \\nWas this helpful? Yes No\\nHere, we define a class MyCustomNode that encapsulates custom logic and interacts with the LLM. This provides a more structured and maintainable way to implement complex node behaviors.\\nEdge types\\nLangGraph supports different edge types to handle various communication patterns between nodes. One useful type is the conditional edge, which allows for decision-making based on a node\\'s output.\\nTo create a conditional edge, you need three components:\\n\\nThe upstream node: The node\\'s output decides the next step.\\nA function: This function evaluates the upstream node\\'s output and determines the next node to execute, returning a string that represents the decision.\\nA mapping: This mapping links the possible outcomes of the function to the corresponding nodes to be executed.\\n\\nHere\\'s an example in pseudocode:\\ngraph.add_conditional_edge(\\n    \"model\",\\n    should_continue,\\n    {\\n        \"end\": END,\\n        \"continue\": \"tools\"\\n    }\\n)\\nPowered By \\nWas this helpful? Yes No\\nHere, after the “model” node is called, we can either exit the graph (”end”) and return to the user, or we can continue (”continue”) and call a tool—depending on what the user decides!\\nState management\\nLangGraph offers powerful state management techniques, which include using external databases like SQLite, PostgreSQL, and MongoDB, or cloud storage solutions like Amazon S3, Google Cloud Storage, and Azure Blob Storage to store and retrieve your agent\\'s state, enabling reliability and scalability.\\nHere\\'s an example of using a SQLite database for state management:\\n```\\nfrom langgraph.checkpoint.sqlite import SqliteSaver\\nConnect to the SQLite database\\nmemory = SqliteSaver.from_conn_string(\":memory:\")\\nCompile the graph with the checkpointer\\ngraph = graph_builder.compile(checkpointer=memory)\\n```\\nPowered By \\nWas this helpful? Yes No\\nError handling\\nLangGraph also provides mechanisms for error handling:\\n\\nExceptions: Node functions can raise exceptions to signal errors during execution. You can catch and handle these exceptions to prevent your graph from crashing.\\nRetry mechanisms: You can implement retry logic within your nodes to handle transient errors, such as network issues or API timeouts.\\nLogging: Use logging to record errors and track the execution of your graph.\\n\\nReal-World Applications of LangGraph\\nLangGraph can be used to build a wide range of applications.\\nChatbots\\nLangGraph is ideal for developing sophisticated chatbots that can handle a wide array of user requests. By leveraging multiple LLM agents, these chatbots can process natural language queries, provide accurate responses, and seamlessly switch between different conversation topics. The ability to manage state and coordinate interactions ensures that the chatbot maintains context and delivers a coherent user experience.\\nAutonomous agents\\nFor applications requiring autonomous decision-making, LangGraph enables the creation of agents that can perform tasks independently based on user inputs and predefined logic.\\nThese agents can execute complex workflows, interact with other systems, and adapt to new information dynamically. LangGraph\\'s structured framework ensures that each agent operates efficiently and effectively, making it suitable for tasks like automated customer support, data processing, and system monitoring.\\nMulti-Agent systems\\nLangGraph excels in building applications where multiple agents collaborate to achieve a common goal. For example, different agents can manage inventory, process orders, and coordinate deliveries in a supply chain management system. LangGraph\\'s coordination capabilities ensure that each agent communicates effectively, sharing information and making decisions in a synchronized manner. This leads to more efficient operations and better overall system performance.\\nWorkflow automation tools\\nWith LangGraph, automating business processes and workflows becomes straightforward. Intelligent agents can be designed to handle tasks such as document processing, approval workflows, and data analysis. By defining clear workflows and leveraging LangGraph\\'s state management, these tools can execute complex sequences of actions without human intervention, reducing errors and increasing productivity.\\nRecommendation systems\\nPersonalized recommendation systems can greatly benefit from LangGraph\\'s capabilities. By employing multiple agents to analyze user behavior, preferences, and contextual data, these systems can deliver tailored suggestions for products, content, or services. LangGraph\\'s flexibility allows for integrating various data sources and algorithms, enhancing the accuracy and relevance of recommendations.\\nPersonalized learning environments\\nIn educational platforms, LangGraph can be used to create adaptive learning environments that cater to individual learning styles and needs. Multiple agents can assess a student\\'s progress, provide customized exercises, and offer real-time feedback. The stateful nature of LangGraph ensures that the system retains information about each learner\\'s performance and preferences, enabling a more personalized and effective educational experience.\\nConclusion\\nLangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.\\nPotential developments for LangGraph include integration with other LangChain components, support for new LLM models, and the introduction of more advanced agent runtimes from academia.\\nIf you want to learn more about developing applications within the LangChain ecosystem, I recommend this course on developing LLM applications with LangChain.\\n\\n\\nAuthor\\nRyan Ong\\n\\nRyan is a lead data scientist specialising in building AI applications using LLMs. He is a PhD candidate in Natural Language Processing and Knowledge Graphs at Imperial College London, where he also completed his Master’s degree in Computer Science. Outside of data science, he writes a weekly Substack newsletter,\\xa0The Limitless Playbook, where he shares one actionable idea from the world\\'s top thinkers and occasionally writes about core AI concepts.\\nTopics\\nPython\\n\\n\\nRyan OngI write about AI research, entrepreneurship, and self-development.\\n\\nTopics\\nPython\\n### LangGraph Studio Guide: Installation, Set Up, Use Cases\\n\\n### Introduction to LangChain for Data Engineering & Data Applications\\n\\n### How to Build LLM Applications with LangChain Tutorial\\n### Building LangChain Agents to Automate Tasks in Python\\n\\n### Building Context-Aware Chatbots: Leveraging LangChain Framework for ChatGPT\\n\\n### Building AI Applications with LangChain and GPT\\nLearn AI with these courses!\\nCourse\\nDeveloping LLM Applications with LangChain\\n3 hr\\n12.7K\\nDiscover how to build AI-powered applications using LLMs, prompts, chains, and agents in LangChain.\\nSee DetailsStart Course\\nCourse\\nDeveloping LLM Applications with LangChain\\n3 hr\\n12.7K\\nDiscover how to build AI-powered applications using LLMs, prompts, chains, and agents in LangChain.\\nSee DetailsStart Course\\nTrack\\nAI Business Fundamentals\\n11hrs hr\\nAccelerate your AI journey, conquer ChatGPT, and develop a comprehensive Artificial Intelligence strategy.\\nSee DetailsStart Course\\nSee More\\nRelated\\nTutorial ### LangGraph Studio Guide: Installation, Set Up, Use Cases\\nLangGraph Studio is a visual development environment for LangChain’s LangGraph framework, simplifying the development of complex agentic applications built with LangChain components.\\n\\nDr Ana Rojo-Echeburúa\\n8 min\\n\\nTutorial ### Introduction to LangChain for Data Engineering & Data Applications\\nLangChain is a framework for including AI from large language models inside data pipelines and applications. This tutorial provides an overview of what you can do with LangChain, including the problems that LangChain solves and examples of data use cases.\\n\\nRichie Cotton\\n11 min\\n\\nTutorial ### How to Build LLM Applications with LangChain Tutorial\\nExplore the untapped potential of Large Language Models with LangChain, an open-source Python framework for building advanced AI applications.\\n\\nMoez Ali\\n12 min\\nTutorial ### Building LangChain Agents to Automate Tasks in Python\\nA comprehensive tutorial on building multi-tool LangChain agents to automate tasks in Python using LLMs and chat models using OpenAI.\\n\\nBex Tuychiev\\n14 min\\n\\nTutorial ### Building Context-Aware Chatbots: Leveraging LangChain Framework for ChatGPT\\nExplore how to build context-aware chatbots using the ChatGPT and LangChain framework.\\n\\nAndrea Valenzuela\\n15 min\\n\\ncode-along ### Building AI Applications with LangChain and GPT\\nIn the live training, you\\'ll use LangChain to build a simple AI application, including preparing and indexing data, prompting the AI, and generating responses.\\n\\nEmmanuel Pire\\nGrow your data skills with DataCamp for Mobile\\nMake progress on the go with our mobile courses and daily 5-minute coding challenges.\\nDownload on the App StoreGet it on Google Play\\nLearn\\nLearn PythonLearn AILearn Power BILearn Data EngineeringAssessmentsCareer TracksSkill TracksCoursesData Science Roadmap\\nData Courses\\nPython CoursesR CoursesSQL CoursesPower BI CoursesTableau CoursesAlteryx CoursesAzure CoursesAWS CoursesGoogle Sheets CoursesExcel CoursesAI CoursesData Analysis CoursesData Visualization CoursesMachine Learning CoursesData Engineering CoursesProbability & Statistics Courses\\nDataLab\\nGet StartedPricingSecurityDocumentation\\nCertification\\nCertificationsData ScientistData AnalystData EngineerSQL AssociatePower BI Data AnalystTableau Certified Data AnalystAzure FundamentalsAI Fundamentals\\nResources\\nResource CenterUpcoming EventsBlogCode-AlongsTutorialsDocsOpen SourceRDocumentationBook a Demo with DataCamp for BusinessData Portfolio\\nPlans\\nPricingFor StudentsFor BusinessFor UniversitiesDiscounts, Promos & SalesDataCamp Donates\\nFor Business\\nBusiness PricingTeams PlanData & AI Unlimited PlanCustomer StoriesPartner Program\\nAbout\\nAbout UsLearner StoriesCareersBecome an InstructorPressLeadershipContact UsDataCamp EspañolDataCamp PortuguêsDataCamp DeutschDataCamp Français\\nSupport\\nHelp CenterBecome an Affiliate\\nFacebookTwitterLinkedInYouTubeInstagram\\nPrivacy PolicyCookie NoticeDo Not Sell My Personal InformationAccessibilitySecurityTerms of Use\\n© 2025 DataCamp, Inc. All Rights Reserved.\\n\\n\\n\\nEnroll Enroll Learn the\\nfundamentals of AI Check Out All AI Courses ✕ Perfect thank you! X \"\" has been copied to the clipboardShake To Reaveal Voucher [close] X Click me for great offers... Thanks... your code is: X'}, {'title': 'LangGraph Tutorial for Beginners - Analytics Vidhya', 'url': 'https://www.analyticsvidhya.com/blog/2025/05/langgraph-tutorial-for-beginners/', 'content': 'Now that we have built the simplest graph in the above section, in this section,\\xa0I will show you how to use LangGraph to build a support chatbot, starting with basic functionality and progressively adding features like web search, memory, and human-in-loop. A. LangGraph offers:– State management which keeps track of data as the agent performs tasks.– Multi-agent support which allows multiple agents to work together within a graph.– Persistence with checkpointers as it saves the state at each step which enable error recovery and debudding.– Human-in-the-loop which helps in pausing the workflow for human review and approval. It allows us to build applications that use the power of LLMs, such as chatbots and AI assistants, while managing complex workflows and state across multiple agents.', 'score': 0.7823471, 'raw_content': 'Master Generative AI with 10+ Real-world Projects in 2025!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReading list\\n\\nData analyst Learning Path\\n\\nTableau Learning Path\\n\\nNLP Learning Path\\n\\nData Scientist Learning Path\\n\\nData Engineer Learning Path\\n\\nMLOps Learning Path\\n\\nAI Engineer Learning Path\\n\\nComputer Vision Learning Path\\n\\nGenerative AI Learning Path\\n\\nGenerative AI Roadmap for Enterprises\\n\\nLLMs Roadmap\\n\\nPrompt Engineer Leaning Path\\n\\nLangGraph Tutorial for Beginners\\n\\nBuilding applications with large language models (LLMs) is exciting, as it lets us create smart, interactive systems. However, making these apps more complex brings along challenges, especially when several LLMs work together. So, how do we manage the flow of information between them? How do we make sure they work smoothly and understand the task? LangGraph is the answer to all such questions. This free tutorial is a great way for beginners to understand how LangGraph can solve these problems. With hands-on examples and complete code, this guide will teach you how to manage multiple LLMs effectively, making your applications more powerful and efficient.\\n\\nTable of Contents\\n\\nUnderstanding LangGraph\\n\\nLangGraph is a powerful library, which is a part of LangChain tools. It helps streamline the integration of LLMs, ensuring they work together seamlessly to understand and execute tasks. It offers a neat way to build and handle LLM apps with many agents.\\n\\nLangGraph lets developers set up how multiple LLM agents talk to each other. It shows these workflows as graphs with cycles. This helps in keeping the communication smooth and performing complex tasks well. LangGraph is best when using Directed Acyclic Graphs (DAGs) for straight line tasks. But since it is cyclic and adds the ability to loop back, it allows for more complex and flexible systems. It’s like how a smart agent might rethink things and use new information to update responses or change its choices.\\n\\nAlso Read: What is LangGraph?\\n\\nKey Concepts of LangGraph\\n\\nHere are some of the key concepts of LangGraph that you need to know:\\n\\n1. Graph Structures\\n\\nLangGraph’s core idea is using a graph for the application’s workflow. This graph has two main parts – nodes and edges.\\n\\n2. State Management\\n\\nKeeping track of what’s happening is vital when you have many agents. All agents need to know the current status of the task. LangGraph handles this by managing the state automatically. The library keeps track of and updates a main state object. It does this as the agents do their jobs. The state object holds important information. It’s available at different points in the workflow. This could include the chat history.\\n\\nIn a chatbot, the state can save the conversation. This helps the bot respond using what was said before. It can also store context data, like user likes, past actions, etc. or external data. Agents can use this for making choices. Internal variables can also be kept here. Agents might use the state to track flags, counts, or other values. These help guide their actions and decisions.\\n\\n3. Multi-agent Systems\\n\\nA multi-agent system consists of multiple independent agents that work together or compete to achieve a common goal. These agents use LLMs to make decisions and control the flow of an application. The complexity of a system can grow as more agents and tasks are added. This may lead to challenges like poor decision making, context management, and the need for specialization. A multi-agent system solves these problems by breaking the system into smaller agents, each focusing on a specific task, such as planning or research.\\n\\nThe main benefits of using a multi-agent system is modularity, specialization, and control. Modularity is for easy development, testing and maintenance, while specialization ensures that expert agents improve overall performance. Control ensures that you can clearly tell how the agents should communicate.\\n\\nAlso Read: A Comprehensive Guide to Building Agentic RAG Systems with LangGraph\\n\\nArchitectures in Multi-agent Systems\\n\\nHere are the various types of architectures followed in multi-agent systems.\\n\\n1. Network Architecture:\\xa0In this architecture, every agent communicates with every other agent, and each can then decide which agent they should call next. This is very helpful when there is no clear sequence of operations. Below is a simple example of how it works using StateGraph.\\n\\n2. Supervisor Architecture: A supervisor agent controls the decision making process and routes tasks to the appropriate agents. Here’s a sample of how it’s done:\\n\\n3. Supervisor with Tool-calling: In this architecture, a supervisor agent uses a tool-calling agent to decide which tool (or agent) to use. The tool executes tasks and returns results that guide the next control flow decision.. A common pattern here is to have a tool-wrapped function:\\n\\n4. Hierarchical Architecture: This approach addresses the complexity of multi-agent systems by organizing agents into teams, each with its own supervisor. The top-level supervisor directs which team to call. For instance:\\n\\n5. Handoffs in Multi-agent Systems: Handoffs allow one agent to pass control to another, facilitating a flow from one agent to the next. Each agent returns a Command object that specifies the next agent to call and send any updates to the state.\\n\\nIn complex systems, agents may be nested within subgraphs, where a node in a subgraph can direct control to another agent outside its graph:\\n\\nMulti-agent systems enable modular and specialized designs where agents independently handle tasks and communicate for efficient problem-solving. Architectures like network, supervisor, and hierarchical systems each serve specific needs, while handoffs ensure smooth transitions between agents, maintaining flexibility and control.\\n\\nDo check out this free course to learn more about Building a Collaborative Multi-Agent System with LangGraph.\\n\\n4. Persistence\\n\\nPersistence means saving the progress of a process so that you can come back to it later, even after some interruptions. Each step’s state is saved, which helps with error recovery. It supports human feedback during runs. You can also replay steps to debug or try new paths.\\n\\nIn LangGraph, persistence is done using checkpointers. Here, the graph’s state is saved after every major step and each saved state is called a checkpoint. All the checkpoints are grouped inside a thread (the conversation history for a particular run).\\n\\nCheckpointing is done automatically and you don’t always need to configure it manually. A checkpoint is like a snapshot of the graph’s state that includes:\\n\\nEach graph, while execution, needs a thread ID to group its checkpoints. You can provide this thread id using config: Below is a sample of how it can be done:\\n\\nTo fetch the most recent state within a thread, use the below code:\\n\\nThe below code shows how you can get a specific checkpoint:\\n\\nTo get the state history or fetch all previous states, use this code:\\n\\nYou can also update or edit the state manually at any point, using:\\n\\nAlso Read: How to Build a LangChain Chatbot with Memory?\\n\\n5. Human-in-the-Loop Integration\\n\\nHuman-in-the-loop lets you add human feedback at key steps of an automated LangGraph workflow. This is crucial in certain tasks since LLMs may generate uncertain or risky outputs such as in tool calls, content generation, or decision-making. LangGraph’s interrupt() function makes this possible by pausing the graph, surfacing data to a human, and resuming with their input using the Command(resume=value) method. This enables review, correction, or data entry.\\n\\nHuman-in-the-loop supports patterns like Approve/Reject, Edit State, Provide Input, or Multi-turn Conversations. To use it, define a checkpointer and add an interrupt() inside a node. You can resume the graph using Command after human input.\\n\\nBelow is a sample of how you can use Human-in-the-loop in LangGraph.\\n\\nThis keeps workflows interactive, auditable, and accurate perfect for high-stakes or collaborative AI use cases.\\n\\n6. Streaming\\n\\nLangGraph streams outputs as they are created which lets users see results faster. This improves their experience with LLMs. Streaming helps you build responsive apps by showing you real-time progress. There are 3 main data types to stream: workflow progress, LLM tokens, and custom updates.\\n\\nUse .stream() (sync) or .astream() (async) to stream outputs. You can set stream_mode to control what you get:\\n\\nYou can pass multiple modes like this:\\n\\nUse .astream_events() if you want a full event stream. This is perfect when migrating big apps.\\n\\nPro tip: For real-time UI feedback, use “messages” for token-wise streaming and “updates” for backend state.\\n\\nWhy Use LangGraph?\\n\\nLangGraph is ideal for developers building smart and flexible AI agents. Here’s why:\\n\\nYou can also take this course from the Langchain academy to learn more.\\n\\nBuilding the Simplest Graph\\n\\nNow that we have seen the key components of LangGraph, let’s try to build a basic graph with three nodes and one conditional edge. This simple example shows how to invoke a graph involving the key concepts of State, Nodes, and Edges.\\n\\nStep 1: Define the Graph State\\n\\nThe State defines the data structure which is shared between nodes. It acts like a shared memory that flows through the graph.\\n\\nHere, we have used python’s TypeDict to declare that our state will have a single key called the graph_state, which stores a string.\\n\\nStep 2: Create the Nodes\\n\\nNodes are just simple Python functions. Each one takes in the current state, modifies it, and returns the updated state.\\n\\nThis function appends “I am” to whatever string is in graph_state.\\n\\nHere, these two nodes add an emotional tone of “happy!” or “sad!” to the sentence.\\n\\nStep 3: Add Conditional Logic\\n\\nSometimes you want dynamic behavior, where the next step depends on logic or randomness. That’s what conditional edges enable.\\n\\nThis function randomly picks between node_2 and node_3 with equal probability, simulating a simple mood selector.\\n\\nStep 4: Construct the Graph\\n\\nLet’s bring it all together using LangGraph’s StateGraph class. This is where we define the full graph structure.\\n\\nWe start with the START node and route to node_1. Then, we add a conditional edge from node_1 using decide_mood. After that, the graph continues to either node_2 or node_3 and ends at the END node.\\n\\nThe compile() method performs basic validation, and draw_mermaid_png() lets you visualize the graph as a Mermaid diagram.\\n\\nStep 5: Invoke the Graph\\n\\nFinally, we can run the graph using the invoke() method.\\n\\nThis starts the graph at the START node and initializes graph_state with the sentence “Hi, this is Janvi.”.\\n\\nOutput:\\n\\nThis output shows how state flows and updates through each step of the graph.\\n\\nBuilding a Support Chatbot with LangGraph Using OpenAI\\n\\nNow that we have built the simplest graph in the above section, in this section,\\xa0I will show you how to use LangGraph to build a support chatbot, starting with basic functionality and progressively adding features like web search, memory, and human-in-loop. Along the way, we will see the\\xa0core LangGraph concepts as well.\\n\\nOur goal here is to create a chatbot that can answer questions using web search, remember past conversations, ask a human for help when needed, use a custom state for behavior, and rewind conversation paths (enabled by checkpointing).\\n\\nAlso Read: Build an AI Coding Agent with LangGraph by LangChain\\n\\nSetup\\n\\nBefore building the chatbot, let’s install the necessary packages.\\n\\nThis command installs:\\n\\nWe need to securely provide the OpenAI API key so the application can authenticate and use the GPT models. This function prompts for the key if it’s not already set in the environment.\\n\\nPart 1: Build a Basic Chatbot\\n\\nWe’ll start by creating the simplest form of the chatbot.\\n\\n1. Define State\\n\\nThe state defines the data structure that gets passed between nodes in the graph. Here, we define a state with a single key, messages, which will hold the list of conversation messages.\\n\\n2. Create Graph Builder\\n\\nThe StateGraph object is the entry point for defining the graph structure. It’s initialized with the State definition we just created.\\n\\n3. Add Chatbot Node\\n\\nWe define a Python function chatbot that takes the current state, invokes OpenAI’s GPT model with the messages from the state, and returns the LLM’s response as an update to the messages key in the state.\\n\\n4. Set Entry and Exit Points\\n\\nDefine the entry point (START) and exit point (END) for the graph execution.\\n\\n5. Compile the Graph\\n\\nOnce all nodes and edges are defined, compile the graph structure.\\n\\n6. Visualize (Optional)\\n\\nLangGraph allows visualizing the compiled graph structure. This helps understand the flow of execution. We can visualize the graph using tools like pygraphviz or mermaid.\\n\\n7. Run the Chatbot\\n\\nSet up a loop to interact with the chatbot. It takes user input, packages it into the expected State format ({“messages”: […]}), and uses graph.stream to execute the graph. The stream method returns events as the graph progresses, and we print the assistant’s final message.\\n\\nPart 2: Enhancing the Chatbot with Tools\\n\\nTo make the chatbot more knowledgeable, especially about recent information, we’ll integrate a web search tool (Tavily). This involves enabling the LLM to request tool usage and adding graph components to handle the execution of these tools.\\n\\n1. Install Tool Requirements\\n\\nInstall the necessary library for the Tavily search tool.\\n\\n2. Set Tool API Key\\n\\nConfigure the API key for the Tavily service.\\n\\n3. Define the Tool\\n\\nInstantiate the TavilySearchResults tool, which will return 2 results. This tool will be used by both the LLM and the graph.\\n\\nPart 3: Add Memory to the Chatbot\\n\\nTo enable multi-turn conversations where the bot remembers previous messages, we introduce LangGraph’s checkpointing feature.\\n\\nAdd Checkpointer\\n\\nUse the MemorySaver checkpointer to store the conversation state in memory. For production, you might use a persistent backend like SQLite or Postgres.\\n\\nPart 4: Human-in-the-loop\\n\\nSometimes, the AI agent might need human input before proceeding. We achieve this by creating a tool that pauses the graph’s flow.\\n\\nDefine Human Assistance Tool\\n\\nThis tool pauses the graph and waits for human input before proceeding.\\n\\nDeploying Your LangGraph Applications\\n\\nOnce you have built your LangGraph application, the next thing which you need to do is running the app either on your local machine or cloud platforms for further development and testing. LangGraph provides us with several deployment options which can have different workflows and infrastructure.\\n\\nFor deployment, LangGraph supports several options. The Cloud SaaS model handles everything for you. The Self-Hosted Data Plane lets you run apps in your own cloud while using LangChain’s control plane. With the Self-Hosted Control Plane, you manage everything yourself. Or go with Standalone Containers for full flexibility using Docker.\\n\\nUse Cases of LangGraph\\n\\nLangGraph is used to build interactive and intelligent AI Agents. Let’s explore and see some of its use cases.\\n\\n1. Improved Customer Service: LangGraph is capable of developing advanced chatbots for customer support. These chatbots are able to recall past purchases and customer preferences.With the recalled past they can respond to the queries about the order and can link to humans when necessary. With this the customer’s problem can be solved faster.\\n\\n2. Research Assistant for AI: A research assistant can also be created using LangGraph. It can look for scholarly articles and then highlight important information. The assistant can then extract the information and this information then can be used by researchers and students to gain more insights from various fields.\\n\\n3. Personalized Learning: With LangGraph we can also build personalized or customized learning systems which will adjust the content based on the learner. This will help the learner understand the weaker area and then recommend resources based on that. This creates a personalized learning experience, improving engagement and outcomes.\\n\\n4. Streamlining Business Tasks: LangGraph can also help us in automating business processes. With this document approval and project management can be automated and also the agent can also be used to analyze data. Automation helps in increasing productivity and reduces human error, allowing teams to focus on higher-level tasks.\\n\\nLearn More: Dynamic AI Workflows Through LangGraph ReAct Function Calling\\n\\nConclusion\\n\\nIn this LangGraph tutorial for beginners, you learned how to build interactive AI systems. These systems go beyond simple Q&A bots. Through LangGraph examples, we saw how LangGraph manages state, integrates multiple agents, and allows human input. The guide showed how to build a support chatbot that can handle web searches, remember past interactions, and even involve human intervention.\\n\\nThe LangGraph tutorial for beginners is very good\\xa0 for developers. It helps create powerful, AI-driven applications. By using LangGraph, we can build flexible, adaptive systems that can handle complex tasks. Whether you’re building a chatbot, research assistant, or personalized learning tool, LangGraph has the structure and tools you need for efficient development.\\n\\nFrequently Asked Questions\\n\\nA. LangGraph is a powerful library that allows developers to make complex and\\xa0advanced AI agents which can interact with large language models. It also helps in managing workflow using graph structure. With the help of this graph structure multiple agents can be built to handle complex tasks.\\n\\nA. LangGraph works by defining workflows as graphs. The graph consists of nodes (tasks or computations) and edges (connections between tasks). It handles state management, making sure each agent has the information it needs to perform its task and interact with other agents.\\n\\nA. LangGraph offers:– State management which keeps track of data as the agent performs tasks.– Multi-agent support which allows multiple agents to work together within a graph.– Persistence with checkpointers as it saves the state at each step which enable error recovery and debudding.– Human-in-the-loop which helps in pausing the workflow for human review and approval.\\n\\nA. Yes, LangGraph can be very easily integrated with OpenAI’s GPT models. It allows us to build applications that use the power of LLMs, such as chatbots and AI assistants, while managing complex workflows and state across multiple agents.\\n\\nA. Yes, this LangGraph tutorial for beginners is designed to help you get started. It walks through key concepts with LangGraph examples and explains how to build systems step by step. Additionally, the LangGraph tutorial for beginners free provides resources for learning the framework at no cost.\\n\\n\\n\\nHi, I am Janvi, a passionate data science enthusiast currently working at Analytics Vidhya. My journey into the world of data began with a deep curiosity about how we can extract meaningful insights from complex datasets.\\n\\nLogin to continue reading and enjoy expert-curated content.\\n\\nFree Courses\\n\\nGenerative AI - A Way of Life\\n\\nExplore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.\\n\\nGetting Started with Large Language Models\\n\\nMaster Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple.\\n\\nBuilding LLM Applications using Prompt Engineering\\n\\nThis free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.\\n\\nImproving Real World RAG Systems: Key Challenges & Practical Solutions\\n\\nExplore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.\\n\\nMicrosoft Excel: Formulas & Functions\\n\\nMaster MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course.\\n\\nRecommended Articles\\n\\nGPT-4 vs. Llama 3.1 – Which Model is Better?\\n\\nLlama-3.1-Storm-8B: The 8B LLM Powerhouse Surpa...\\n\\nA Comprehensive Guide to Building Agentic RAG S...\\n\\nTop 10 Machine Learning Algorithms in 2025\\n\\n45 Questions to Test a Data Scientist on Basics...\\n\\n90+ Python Interview Questions and Answers (202...\\n\\n8 Easy Ways to Access ChatGPT for Free\\n\\nPrompt Engineering: Definition, Examples, Tips ...\\n\\nWhat is LangChain?\\n\\nWhat is Retrieval-Augmented Generation (RAG)?\\n\\nResponses From Readers\\n\\nClearSubmit reply\\n\\n\\n\\nΔ\\n\\nWrite for us\\n\\nWrite, captivate, and earn accolades and rewards for your work\\n\\nFlagship Programs\\n\\nFree Courses\\n\\nPopular Categories\\n\\nGenerative AI Tools and Techniques\\n\\nPopular GenAI Models\\n\\nAI Development Frameworks\\n\\nData Science Tools and Techniques\\n\\nCompany\\n\\nDiscover\\n\\nLearn\\n\\nEngage\\n\\nContribute\\n\\nEnterprise\\n\\nTerms & conditions\\n\\n\\n\\nRefund Policy\\n\\n\\n\\nPrivacy Policy\\n\\n\\n\\nCookies Policy\\n© Analytics Vidhya 2025.All rights reserved.\\n\\nContinue your learning for FREE\\n\\nEnter email address to continue\\n\\n\\n\\nEnter OTP sent to\\n\\nEdit\\n\\nEnter the OTP\\n\\nResend OTP\\n\\nResend OTP in 45s\\n\\n'}, {'title': 'Langgraph tutorial - YouTube', 'url': 'https://www.youtube.com/playlist?list=PLplW4d4HPsEIjChXF8UdJV9rXmuBtD1MS', 'content': 'Learn how to use Langgraph, a software development tool for creating chatbots and applications with LLMs, in this step-by-step guide. Watch 21 videos covering topics such as environment setup, nodes, tools, memory, graph studio, human in the loop, and more.', 'score': 0.7568076, 'raw_content': None}, {'title': 'Introduction to LangGraph', 'url': 'https://academy.langchain.com/courses/intro-to-langgraph', 'content': 'Lesson 1: Motivation Lesson 2: Simple Graph Lesson 3: LangGraph Studio Lesson 4: Chain Lesson 5: Router Lesson 6: Agent Lesson 7: Agent with Memory [Optional] Lesson 8: Intro to Deployment Lesson 1: State Schema Lesson 2: State Reducers Lesson 3: Multiple Schemas Lesson 5: Chatbot w/ Summarizing Messages and Memory Lesson 1: Streaming Lesson 2: Breakpoints Lesson 3: Editing State and Human Feedback Lesson 4: Dynamic Breakpoints Lesson 1: Parallelization Lesson 4: Research Assistant Lesson 2: LangGraph Store Lesson 3: Memory Schema + Profile Lesson 4: Memory Schema + Collection Lesson 5: Build an Agent with Long-Term Memory Lesson 1: Deployment Concepts Lesson 2: Creating a Deployment Lesson 3: Connecting to a Deployment Lesson 5: Assistants About this course', 'score': 0.7211353, 'raw_content': \"LangChain Academy\\n\\nIntroduction to LangGraph\\n\\nLearn the basics of LangGraph - our framework for building agentic and multi-agent applications. Separate from the LangChain package, LangGraph helps developers add better precision and control into agentic workflows.\\n\\nCourse Curriculum\\n\\nWelcome to the course!\\n\\nCourse Overview\\n\\nGetting Set Up\\n\\nGetting Set Up (Video Guide)\\n\\nModule 0 Resources\\n\\nModule 1: Introduction\\n\\nModule 1 Introduction\\n\\nModule 1 Resources\\n\\nLesson 1: Motivation\\n\\nLesson 2: Simple Graph\\n\\nLesson 3: LangGraph Studio\\n\\nLesson 4: Chain\\n\\nLesson 5: Router\\n\\nLesson 6: Agent\\n\\nLesson 7: Agent with Memory\\n\\n[Optional] Lesson 8: Intro to Deployment\\n\\nModule 1 Feedback\\n\\nModule 2: State and Memory\\n\\nModule 2 Introduction\\n\\nModule 2 Resources\\n\\nLesson 1: State Schema\\n\\nLesson 2: State Reducers\\n\\nLesson 3: Multiple Schemas\\n\\nLesson 4: Trim and Filter Messages\\n\\nLesson 5: Chatbot w/ Summarizing Messages and Memory\\n\\nLesson 6: Chatbot w/ Summarizing Messages and External Memory\\n\\nModule 2 Feedback\\n\\nModule 3: UX and Human-in-the-Loop\\n\\nModule 3 Introduction\\n\\nModule 3 Resources\\n\\nLesson 1: Streaming\\n\\nLesson 2: Breakpoints\\n\\nLesson 3: Editing State and Human Feedback\\n\\nLesson 4: Dynamic Breakpoints\\n\\nLesson 5: Time Travel\\n\\nModule 3 Feedback\\n\\nModule 4: Building Your Assistant\\n\\nModule 4 Introduction\\n\\nModule 4 Resources\\n\\nLesson 1: Parallelization\\n\\nLesson 2: Sub-graphs\\n\\nLesson 3: Map-reduce\\n\\nLesson 4: Research Assistant\\n\\nModule 4 Feedback\\n\\nModule 5: Long-Term Memory\\n\\nModule 5 Resources\\n\\nLesson 1: Short vs. Long-Term Memory\\n\\nLesson 2: LangGraph Store\\n\\nLesson 3: Memory Schema + Profile\\n\\nLesson 4: Memory Schema + Collection\\n\\nLesson 5: Build an Agent with Long-Term Memory\\n\\nModule 5 Feedback\\n\\nModule 6: Deployment\\n\\nModule 6 Resources\\n\\nLesson 1: Deployment Concepts\\n\\nLesson 2: Creating a Deployment\\n\\nLesson 3: Connecting to a Deployment\\n\\nLesson 4: Double Texting\\n\\nLesson 5: Assistants\\n\\nModule 6 Feedback\\n\\nEnd of Course Feedback\\n\\nAbout this course\\n\\nLangGraph FAQs\\n\\nNo. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. On the other hand, LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.\\n\\nOther agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company’s needs. LangGraph provides a more expressive framework to handle companies’ unique tasks without restricting users to a single black-box cognitive architecture.\\n\\nLangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.\\n\\nYes. LangGraph is an MIT-licensed open-source library and is free to use.\\n\\nNo. LangGraph Cloud is proprietary software that will eventually be a paid service for certain tiers of usage. We will always give ample notice before charging for a service and reward our early adopters with preferential pricing.\\n\\nDeploy agents at scale, monitor carefully, iterate boldly\\n\\nDesign agent-driven user experiences with LangGraph Platform's APIs. Quickly deploy and scale your application with infrastructure built for agents. Choose from multiple deployment options.\\n\\nReady to start shipping reliable GenAI apps faster?\\n\\nGet started with LangGraph and LangSmith to enhanceyour LLM app development, from prototype to production.\\n\\nBring LangChain Academy to Your Company\\n\\nNominate your company to receive a LangChain Academy training, available both in-person at your office or remotely, at no cost.\\n\\n\"}, {'title': 'How to Build AI Agents with LangGraph: A Step-by-Step Guide', 'url': 'https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832', 'content': 'In this step, we’ll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the user’s input and tool output. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). This step ensures that the AI assistant can access and trigger the tools as needed during the conversation, creating a seamless interaction between the user and the assistant. By following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs.', 'score': 0.61834323, 'raw_content': \"Sign up\\n\\nSign in\\n\\nSign up\\n\\nSign in\\n\\nHow to Build AI Agents with LangGraph: A Step-by-Step Guide\\n\\n--\\n\\n21\\n\\nListen\\n\\nShare\\n\\nIntroduction\\n\\nIn the world of AI, retrieval-augmented generation (RAG) systems have become common tools for handling simple queries and generating contextually relevant responses. However, as the demand for more sophisticated AI applications grows, there’s a need for systems that go beyond these retrieval capabilities. Enter AI agents — autonomous entities capable of performing complex, multi-step tasks, maintaining state across interactions, and dynamically adapting to new information. LangGraph, a powerful extension of the LangChain library, is designed to help developers build these advanced AI agents by enabling stateful, multi-actor applications with cyclic computation capabilities.\\n\\nIn this article, we’ll explore how LangGraph transforms AI development and provide a step-by-step guide on how to build your own AI agent using an example that computes energy savings for solar panels. This example will showcase how LangGraph’s unique features can create intelligent, adaptable, and real-world-ready AI systems.\\n\\nWhat is LangGraph?\\n\\nLangGraph is an advanced library built on top of LangChain, designed to enhance your Large Language Model (LLM) applications by introducing cyclic computational capabilities. While LangChain allows the creation of Directed Acyclic Graphs (DAGs) for linear workflows, LangGraph takes this a step further by enabling the addition of cycles, which are essential for developing complex, agent-like behaviors. These behaviors allow LLMs to continuously loop through a process, dynamically deciding what action to take next based on evolving conditions.\\n\\nAt the heart of LangGraph is the concept of a stateful graph:\\n\\nLangGraph redefines AI development by seamlessly managing graph structure, state, and coordination, empowering the creation of sophisticated, multi-actor applications. With automatic state management, LangGraph ensures that context is preserved across interactions, enabling your AI to respond intelligently to changing inputs. Its streamlined agent coordination guarantees precise execution and efficient information exchange, letting you focus on crafting innovative workflows rather than technical intricacies. LangGraph’s flexibility allows for the development of tailored, high-performance applications, while its scalability and fault tolerance ensure your systems remain robust and reliable, even at the enterprise level.\\n\\nStep-by-step Guide\\n\\nNow that we have a solid understanding of what LangGraph is and how it enhances AI development, let’s dive into a practical example. In this scenario, we’ll build an AI agent designed to calculate potential energy savings for solar panels based on user input. This agent can be implemented as a lead generation tool on a solar panel seller’s website, where it interacts with potential customers, offering personalized savings estimates. By gathering key data such as monthly electricity costs, this AI agent helps educate customers on the financial benefits of solar energy while simultaneously qualifying leads for follow-up by the sales team. This example showcases the power of LangGraph in creating intelligent, dynamic systems that can automate complex tasks and drive business value.\\n\\nStep 1: Import Necessary Libraries\\n\\nWe start by importing all the essential Python libraries and modules required for the project.\\n\\nThese imports set the foundation for utilizing LangChain, LangGraph, and AWS services to build our AI assistant.\\n\\nStep 2: Define the Tool for Calculating Solar Savings\\n\\nNext, we define a tool that will handle the computation of energy savings based on the monthly electricity cost provided by the user.\\n\\nThis function processes the user’s monthly electricity cost and returns a detailed estimate of the solar panel system’s benefits, including the number of panels required, installation costs, and net savings over ten years. For simplicity, we have made a few assumptions in the calculations, such as the average cost per kilowatt-hour and average sunlight hours. However, in a more advanced version of this AI agent, we could gather this information directly from the user, tailoring the estimates more precisely to their unique circumstances.\\n\\nStep 3: Set Up State Management and Error Handling\\n\\nEffective state management and error handling are crucial for building robust AI systems. Here, we define utilities to manage errors and maintain the conversation’s state.\\n\\nThese functions ensure that any errors encountered during the tool’s execution are handled gracefully, providing helpful feedback to the user.\\n\\nStep 4: Define the State and Assistant Class\\n\\nIn this step, we’ll define how the AI agent manages its state (the ongoing context of the conversation) and ensure it responds appropriately to the user’s input and tool output.\\n\\nTo do this, we create a State class using Python's TypedDict to define the structure of the messages that will be passed around. The state will hold messages, including input from the user and output from the agent or tools.\\n\\nNext, we create the Assistant class, which is responsible for running the AI agent, interacting with the tools, and managing the flow of the conversation. The Assistant invokes the tools, ensures they return appropriate results, and handles any re-prompts or errors that may occur during execution. Its core functionality includes invoking the Runnable, which defines the process of calling the LLM and tools like compute_savings, and then monitoring the results. If the agent fails to return a valid response or if a tool doesn't provide meaningful data, the Assistant re-prompts the user or requests clarification. It continues to loop through the Runnable until a valid output is obtained, ensuring smooth execution and effective responses.\\n\\nThis setup is essential for maintaining the flow of conversation and ensuring that the assistant responds appropriately based on the context.\\n\\nStep 5: Set Up the LLM with AWS Bedrock\\n\\nIn this step, we configure the Large Language Model (LLM) using AWS Bedrock, which will power the AI assistant’s language capabilities. AWS Bedrock allows us to access advanced LLMs such as Anthropic’s Claude. To interact with AWS services, you need to have your AWS credentials configured. This means you must either have your AWS credentials set in your environment (through the AWS CLI or environment variables) or use a credentials file that AWS SDKs can access. Without proper AWS configuration, the assistant won’t be able to connect to AWS services like Bedrock for running the LLM.\\n\\nThis integration ensures that the assistant can effectively interpret and respond to user inputs.\\n\\nStep 6: Define the Assistant’s Workflow\\n\\nNow that we have set up the LLM and tools, the next step is to define the AI assistant’s workflow. This involves creating a template for the conversation, specifying the tools that the assistant will use, and configuring how the AI agent will respond to user input and trigger different functions (like calculating solar savings). The workflow is essentially the logic that governs how the assistant interacts with users, gathers information, and calls tools to provide results.\\n\\nThe first part of the workflow involves creating a prompt template that defines how the assistant will communicate with the user. The prompt helps guide the AI assistant in determining what to ask the user, how to respond based on the input, and when to trigger tools like compute_savings.\\n\\nIn this case, the assistant needs to ask the user for their monthly electricity cost to calculate solar panel savings. Here’s how we define the conversation:\\n\\nNext, we define the tools that the assistant will use during the interaction, with the primary tool being compute_savings, which calculates potential savings based on the user's monthly electricity cost. After specifying the tools in the list, we bind them to the assistant's workflow using the llm.bind_tools() method. This step ensures that the AI assistant can access and trigger the tools as needed during the conversation, creating a seamless interaction between the user and the assistant.\\n\\nStep 7: Build the Graph Structure\\n\\nIn this step, we construct the graph structure for the AI assistant using LangGraph, which controls how the assistant processes user input, triggers tools, and moves between stages. The graph defines nodes for the core actions (like invoking the assistant and tool) and edges that dictate the flow between these nodes.\\n\\nEach node in LangGraph represents an operational step, such as interacting with the user or executing a tool. We define two key nodes for this AI assistant:\\n\\nEdges define how the flow moves between nodes. Here, the assistant starts the conversation, then transitions to the tool once the required input is collected, and returns to the assistant after the tool’s execution.\\n\\nWe use MemorySaver to ensure the graph retains the conversation state across different steps. This allows the assistant to remember the user’s input, ensuring continuity in multi-step interactions.\\n\\nStep 8: Running the Assistant\\n\\nFinally, you can run the assistant by initiating the graph and starting the conversation.\\n\\nConclusion\\n\\nBy following these steps, you have successfully created an AI assistant using LangGraph that can calculate solar panel energy savings based on user inputs. This tutorial demonstrates the power of LangGraph in managing complex, multi-step processes and highlights how to leverage advanced AI tools to solve real-world challenges efficiently. Whether you’re developing AI agents for customer support, energy management, or other applications, LangGraph provides the flexibility, scalability, and robustness needed to bring your ideas to life.\\n\\nInterested in visualizing and testing this agent using LangGraph Studio? Check out my recent article and YouTube video:\\n\\nLangGraph Studio: Visualizing and Testing AI Agents with LangChain\\n\\nA guide to using LangGraph Studio for visualizing, testing, and developing AI agents with LangChain. Includes setup and…\\n\\nmedium.com\\n\\nFollow me for more AI deep dives!\\n\\nMedium, Instagram, YouTube\\n\\n--\\n\\n--\\n\\n21\\n\\nWritten by Lore Van Oudenhove\\n\\nYoung Entrepreneur, Data Scientist, and AI enthusiast. On the journey of building my own AI startup.\\n\\nResponses (21)\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech\\n\\n\"}]\n"
     ]
    }
   ],
   "source": [
    "# 도구 실행\n",
    "print(tool.invoke(\"LangGraph Tutorial\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c96daa",
   "metadata": {},
   "source": [
    "#### LLM + Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "eb8fb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Optional, Dict\n",
    "\n",
    "class StartupCandidate(TypedDict):\n",
    "    name: str\n",
    "    summary: Optional[str]\n",
    "\n",
    "class MarketScores(TypedDict):\n",
    "    market_size: int\n",
    "    problem_fit: int\n",
    "    willingness_to_pay: int\n",
    "    revenue_model_clarity: int\n",
    "    upside_potential: int\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    domain: str\n",
    "    startup_candidates: List[StartupCandidate]\n",
    "    current_index: int\n",
    "    candidates_documents: List[StartupCandidate]\n",
    "    evaluation_scores: Dict[str, MarketScores]\n",
    "    evaluation_summary: Dict[str, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "95e41028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 State 예시\n",
    "input = {\n",
    "    'user_query': 'AI 헬스케어 스타트업 알려줘',\n",
    "    'startup_candidates': [\n",
    "        {\n",
    "            'name': 'Medibot',\n",
    "            'summary': 'Medibot은 AI 기반 진단 시스템으로 빠르고 정확한 진료 보조를 제공. AI 알고리즘을 통해 의료 데이터 분석을 지원.'\n",
    "        }\n",
    "        ,{\n",
    "            'name':'Medibot22',\n",
    "            'summary':'Medibot은 AI 기반 진단 시스템으로 빠르고 정확한 진료 보조를 제공. AI 알고리즘을 통해 의료 데이터 분석을 지원.'\n",
    "        }\n",
    "    ],\n",
    "    'domain': '헬스케어'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1eb7d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "def fetch_tavily_context(startup_name: str) -> str:\n",
    "    search = TavilySearch()\n",
    "    query = f\"{startup_name} 시장성 OR 성장 가능성 OR 투자 현황 OR 경쟁사 비교\"\n",
    "    \n",
    "    try:\n",
    "        results = search.run(query)\n",
    "        if not results:\n",
    "            return f\"{startup_name} 관련 정보가 충분하지 않습니다.\"\n",
    "        return \"\\n\".join(results[:3])[:3000]\n",
    "    except Exception as e:\n",
    "        return f\"Tavily 검색 중 오류 발생: {str(e)}\"\n",
    "\n",
    "def check_context_node(state: AgentState) -> AgentState:\n",
    "    current = state[\"startup_candidates\"][state[\"current_index\"]]\n",
    "    summary = current.get(\"summary\")\n",
    "\n",
    "    if not summary or len(summary.strip()) < 10:\n",
    "        summary = fetch_tavily_context(current[\"name\"])\n",
    "        current[\"summary\"] = summary\n",
    "\n",
    "    state[\"startup_candidates\"][state[\"current_index\"]] = current\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf288839",
   "metadata": {},
   "source": [
    "#### 시장성 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7fa93ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_response(response: str) -> str:\n",
    "    try:\n",
    "        if \"```json\" in response:\n",
    "            return response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        match = re.search(r\"({[\\\\s\\\\S]*})\", response)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] JSON 추출 실패: {e}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9b06deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_market_evaluation(startup_name: str, summary: str, domain: str, context: str = \"\") -> Tuple[Dict[str, int], str]:\n",
    "    prompt = f\"\"\"\n",
    "아래는 스타트업에 대한 개요입니다:\n",
    "\n",
    "[회사명]: {startup_name}\n",
    "[개요]: {summary}\n",
    "\n",
    "이 스타트업의 **시장성**을 다음 항목에 맞춰 분석해 주세요:\n",
    "\n",
    "- 이 시장은 얼마나 큰가? (market_size)\n",
    "- 제품이 시장의 실제 문제를 해결하는가? (problem_fit)\n",
    "- 고객이 실제로 이 제품에 비용을 지불할 이유가 있는가? (willingness_to_pay)\n",
    "- 수익 모델은 명확한가? (revenue_model_clarity)\n",
    "- 이 스타트업이 성공한다면, 정말 큰 기회가 될까? (upside_potential)\n",
    "\n",
    "각 항목에 대해 0~10점으로 평가하고, 요약도 함께 작성해 주세요.\n",
    "\n",
    "반드시 아래 JSON 형식으로 출력하세요:\n",
    "```json\n",
    "{{\n",
    " \"evaluation_scores\": {{\n",
    "    \"market_size\": 0,\n",
    "    \"problem_fit\": 0,\n",
    "    \"willingness_to_pay\": 0,\n",
    "    \"revenue_model_clarity\": 0,\n",
    "    \"upside_potential\": 0\n",
    "  }},\n",
    "  \"evaluation_summary\": \"...\"\n",
    "}}\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    print(\"[DEBUG] LLM 응답 원본:\\n\", response)\n",
    "\n",
    "    try:\n",
    "        json_str = extract_json_from_response(response)\n",
    "        print(\"[DEBUG] 추출된 JSON:\\n\", json_str)\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "        # 강제 검증\n",
    "        scores = parsed.get(\"evaluation_scores\", {})\n",
    "        summary = parsed.get(\"evaluation_summary\", \"\")\n",
    "\n",
    "        if not isinstance(scores, dict):\n",
    "            raise ValueError(\"evaluation_scores is not a dictionary\")\n",
    "\n",
    "        expected_keys = [\n",
    "            \"market_size\", \"problem_fit\", \"willingness_to_pay\",\n",
    "            \"revenue_model_clarity\", \"upside_potential\"\n",
    "        ]\n",
    "        for key in expected_keys:\n",
    "            value = scores.get(key)\n",
    "            if not isinstance(value, int):\n",
    "                raise ValueError(f\"[{key}] is not int: {value} (type={type(value)})\")\n",
    "\n",
    "        return scores, summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] JSON parsing failed: {e}\")\n",
    "        return {\n",
    "            \"market_size\": 5,\n",
    "            \"problem_fit\": 5,\n",
    "            \"willingness_to_pay\": 5,\n",
    "            \"revenue_model_clarity\": 5,\n",
    "            \"upside_potential\": 5\n",
    "        }, \"⚠ JSON 파싱 실패로 기본 점수를 반환합니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf98db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_context_node(state: AgentState) -> AgentState:\n",
    "    current = state[\"startup_candidates\"][state[\"current_index\"]]\n",
    "    summary = current.get(\"summary\", \"\")\n",
    "    if not summary or len(summary.strip()) < 10:\n",
    "        current[\"summary\"] = fetch_tavily_context(current[\"name\"])\n",
    "    state[\"startup_candidates\"][state[\"current_index\"]] = current\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3abf8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_node(state: AgentState) -> AgentState:\n",
    "    candidate = state[\"startup_candidates\"][state[\"current_index\"]]\n",
    "    name = candidate[\"name\"]\n",
    "    print(f\"[INFO] 분석 중: {name}\")\n",
    "\n",
    "    summary = candidate[\"summary\"]\n",
    "    domain = state[\"domain\"]\n",
    "    context = summary\n",
    "\n",
    "    scores, summary_text = generate_market_evaluation(\n",
    "            startup_name=name,\n",
    "            summary=summary,\n",
    "            domain=state[\"domain\"],\n",
    "            context=summary\n",
    "        )\n",
    "\n",
    "    state[\"candidates_documents\"].append(candidate)\n",
    "    state[\"evaluation_scores\"][name] = scores\n",
    "    state[\"evaluation_summary\"][name] = summary_text\n",
    "\n",
    "    state[\"current_index\"] += 1  # ✅ 실제 상태 변경은 여기서\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7541df79",
   "metadata": {},
   "source": [
    "#### 최종정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b30e9aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results_node(state: AgentState) -> AgentState:\n",
    "    return {\n",
    "    \"candidates_documents\": state[\"candidates_documents\"],\n",
    "    \"domain\": state[\"domain\"],\n",
    "    \"evaluation_scores\": state[\"evaluation_scores\"],\n",
    "    \"evaluation_summary\": state[\"evaluation_summary\"]  # ✅ 그대로 dictionary로 유지\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86c0a4",
   "metadata": {},
   "source": [
    "#### Langgraph 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8089091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_and_finish(state: AgentState) -> AgentState:\n",
    "    for candidate in state[\"startup_candidates\"]:\n",
    "        name = candidate[\"name\"]\n",
    "        summary = candidate.get(\"summary\", \"\")\n",
    "        \n",
    "        # 🔍 summary 부족 시 Tavily로 보완\n",
    "        if not summary or len(summary.strip()) < 10:\n",
    "            summary = fetch_tavily_context(name)\n",
    "        candidate[\"summary\"] = summary\n",
    "\n",
    "        # 💡 시장성 평가 수행\n",
    "        scores, summary_text = generate_market_evaluation(\n",
    "            startup_name=name,\n",
    "            summary=summary,\n",
    "            domain=state[\"domain\"],\n",
    "            context=summary\n",
    "        )\n",
    "\n",
    "        # 📌 결과 누적\n",
    "        state[\"candidates_documents\"].append(candidate)\n",
    "        state[\"evaluation_scores\"][name] = scores\n",
    "        state[\"evaluation_summary\"][name] = summary_text\n",
    "\n",
    "        print(f\"[INFO] ✅ 분석 완료: {name}\")\n",
    "\n",
    "    return {\n",
    "        \"candidates_documents\": state[\"candidates_documents\"],\n",
    "        \"domain\": state[\"domain\"],\n",
    "        \"evaluation_scores\": state[\"evaluation_scores\"],\n",
    "        \"evaluation_summary\": state[\"evaluation_summary\"]  # ✅ 그대로 dictionary로 유지\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "531a3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"analyze_all_and_finish\", analyze_all_and_finish)\n",
    "graph.set_entry_point(\"analyze_all_and_finish\")\n",
    "graph.add_edge(\"analyze_all_and_finish\", END)\n",
    "\n",
    "market_analysis_app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1b566184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] LLM 응답 원본:\n",
      " \n",
      "```json\n",
      "{\n",
      " \"evaluation_scores\": {\n",
      "    \"market_size\": 8,\n",
      "    \"problem_fit\": 9,\n",
      "    \"willingness_to_pay\": 9,\n",
      "    \"revenue_model_clarity\": 8,\n",
      "    \"upside_potential\": 9\n",
      "  },\n",
      "  \"evaluation_summary\": \"Medibot은 건강 관리 분야에서의 AI 기반 진단 시스템으로 매우 큰 시장 규모를 대상으로 하고 있습니다. 제품은 실제 진료 과정에서 발생하는 문제를 해결하며, 고객들은 빠르고 정확한 진료 보조에 대해 비용을 지불할 의향이 있습니다. 수익 모델도 명확하고, 성공한다면 큰 기회를 가질 수 있는 스타트업으로 평가됩니다.\"\n",
      "}\n",
      "```\n",
      "[DEBUG] 추출된 JSON:\n",
      " {\n",
      " \"evaluation_scores\": {\n",
      "    \"market_size\": 8,\n",
      "    \"problem_fit\": 9,\n",
      "    \"willingness_to_pay\": 9,\n",
      "    \"revenue_model_clarity\": 8,\n",
      "    \"upside_potential\": 9\n",
      "  },\n",
      "  \"evaluation_summary\": \"Medibot은 건강 관리 분야에서의 AI 기반 진단 시스템으로 매우 큰 시장 규모를 대상으로 하고 있습니다. 제품은 실제 진료 과정에서 발생하는 문제를 해결하며, 고객들은 빠르고 정확한 진료 보조에 대해 비용을 지불할 의향이 있습니다. 수익 모델도 명확하고, 성공한다면 큰 기회를 가질 수 있는 스타트업으로 평가됩니다.\"\n",
      "}\n",
      "[INFO] ✅ 분석 완료: Medibot\n",
      "[DEBUG] LLM 응답 원본:\n",
      " \n",
      "```json\n",
      "{\n",
      " \"evaluation_scores\": {\n",
      "    \"market_size\": 8,\n",
      "    \"problem_fit\": 7,\n",
      "    \"willingness_to_pay\": 6,\n",
      "    \"revenue_model_clarity\": 5,\n",
      "    \"upside_potential\": 9\n",
      "  },\n",
      "  \"evaluation_summary\": \"닥터나우는 시장이 크고, 제품이 실제 문제를 해결하며 고객이 비용을 지불할 의사가 있습니다. 수익 모델은 조금 더 명확해야 하지만, 성공 시 큰 기회가 될 것으로 기대됩니다.\"\n",
      "}\n",
      "```\n",
      "[DEBUG] 추출된 JSON:\n",
      " {\n",
      " \"evaluation_scores\": {\n",
      "    \"market_size\": 8,\n",
      "    \"problem_fit\": 7,\n",
      "    \"willingness_to_pay\": 6,\n",
      "    \"revenue_model_clarity\": 5,\n",
      "    \"upside_potential\": 9\n",
      "  },\n",
      "  \"evaluation_summary\": \"닥터나우는 시장이 크고, 제품이 실제 문제를 해결하며 고객이 비용을 지불할 의사가 있습니다. 수익 모델은 조금 더 명확해야 하지만, 성공 시 큰 기회가 될 것으로 기대됩니다.\"\n",
      "}\n",
      "[INFO] ✅ 분석 완료: 닥터나우\n",
      "{'user_query': 'AI 헬스케어 스타트업 알려줘', 'domain': '헬스케어', 'startup_candidates': [{'name': 'Medibot', 'summary': 'Medibot은 AI 기반 진단 시스템으로 빠르고 정확한 진료 보조를 제공.'}, {'name': '닥터나우', 'summary': '완전 개꿀이야 닥터나우'}], 'current_index': 0, 'candidates_documents': [{'name': 'Medibot', 'summary': 'Medibot은 AI 기반 진단 시스템으로 빠르고 정확한 진료 보조를 제공.'}, {'name': '닥터나우', 'summary': '완전 개꿀이야 닥터나우'}], 'evaluation_scores': {'Medibot': {'market_size': 8, 'problem_fit': 9, 'willingness_to_pay': 9, 'revenue_model_clarity': 8, 'upside_potential': 9}, '닥터나우': {'market_size': 8, 'problem_fit': 7, 'willingness_to_pay': 6, 'revenue_model_clarity': 5, 'upside_potential': 9}}, 'evaluation_summary': {'Medibot': 'Medibot은 건강 관리 분야에서의 AI 기반 진단 시스템으로 매우 큰 시장 규모를 대상으로 하고 있습니다. 제품은 실제 진료 과정에서 발생하는 문제를 해결하며, 고객들은 빠르고 정확한 진료 보조에 대해 비용을 지불할 의향이 있습니다. 수익 모델도 명확하고, 성공한다면 큰 기회를 가질 수 있는 스타트업으로 평가됩니다.', '닥터나우': '닥터나우는 시장이 크고, 제품이 실제 문제를 해결하며 고객이 비용을 지불할 의사가 있습니다. 수익 모델은 조금 더 명확해야 하지만, 성공 시 큰 기회가 될 것으로 기대됩니다.'}}\n"
     ]
    }
   ],
   "source": [
    "input_state = {\n",
    "    \"user_query\": \"AI 헬스케어 스타트업 알려줘\",\n",
    "    \"domain\": \"헬스케어\",\n",
    "    \"startup_candidates\": [\n",
    "        {\"name\": \"Medibot\", \"summary\": \"Medibot은 AI 기반 진단 시스템으로 빠르고 정확한 진료 보조를 제공.\"},\n",
    "        {\"name\": \"닥터나우\", \"summary\": \"완전 개꿀이야 닥터나우\"}\n",
    "    ],\n",
    "    \"current_index\": 0,\n",
    "    \"candidates_documents\": [],\n",
    "    \"evaluation_scores\": {},\n",
    "    \"evaluation_summary\": {}\n",
    "}\n",
    "\n",
    "result = market_analysis_app.invoke(input_state)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a48ed326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__([<p>__start__</p>]):::first\\n\\tanalyze_all_and_finish(analyze_all_and_finish)\\n\\t__end__([<p>__end__</p>]):::last\\n\\t__start__ --> analyze_all_and_finish;\\n\\tanalyze_all_and_finish --> __end__;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_analysis_app.get_graph().draw_mermaid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82742a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
